{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ae313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# PROJECT_ROOT = os.path.dirname(os.path.abspath(\"\"))\n",
    "\n",
    "# if PROJECT_ROOT not in sys.path:\n",
    "#     sys.path.insert(0, PROJECT_ROOT)\n",
    "# from scripts.extract import WorldBankExtractOperator\n",
    "# from scripts.transform import SparkTransformOperator\n",
    "# from scripts.loads import DatabaseLoaderService\n",
    "# from scripts.load_wbapi_etl_config import ETLPipelineConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b5c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kha thi\n",
      "<Response [200]>\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\python_enviroment\\Lib\\site-packages\\requests\\models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\python_enviroment\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\python_enviroment\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\python_enviroment\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mfetch_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkha thi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\python_enviroment\\Lib\\site-packages\\requests\\models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cophieu68 Crawler v·ªõi BeautifulSoup\n",
    "Nhanh h∆°n, ·ªïn ƒë·ªãnh h∆°n, √≠t t√†i nguy√™n h∆°n Selenium\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, Dict, List, Any, Union\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Cophieu68BeautifulSoupCrawler:\n",
    "    \"\"\"Crawler s·ª≠ d·ª•ng BeautifulSoup cho cophieu68.vn\"\"\"\n",
    "    \n",
    "    def __init__(self, delay: float = 1.0, timeout: int = 30):\n",
    "        self.delay = delay\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Setup headers ƒë·ªÉ tr√°nh b·ªã block\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        \n",
    "        # URLs mapping\n",
    "        self.urls = {\n",
    "            \"base_url\": \"https://www.cophieu68.vn\",\n",
    "            \"summary\": \"https://www.cophieu68.vn/quote/summary.php?id=\",\n",
    "            \"financial\": \"https://www.cophieu68.vn/quote/financial.php?id=\",\n",
    "            \"financial_detail\": \"https://www.cophieu68.vn/quote/financial_detail.php?id=\",\n",
    "            \"profile\": \"https://www.cophieu68.vn/quote/profile.php?id=\",\n",
    "            \"history\": \"https://www.cophieu68.vn/quote/history.php?id=\",\n",
    "            \"events\": \"https://www.cophieu68.vn/quote/event.php?id=\",\n",
    "            \"chart\": \"https://www.cophieu68.vn/chart/chart.php?id=\",\n",
    "            \"market_data\": \"https://www.cophieu68.vn/market/markets.php\",\n",
    "            \"categories\": \"https://www.cophieu68.vn/category/category_index.php\",\n",
    "            \"foreigner\": \"https://www.cophieu68.vn/stats/foreigner.php\",\n",
    "            \"volume_buzz\": \"https://www.cophieu68.vn/stats/volume_buzz.php\",\n",
    "            \"stats\": \"https://www.cophieu68.vn/stats/stats.php\",\n",
    "        }\n",
    "        \n",
    "        logger.info(\"‚úÖ Cophieu68 BeautifulSoup Crawler initialized\")\n",
    "    \n",
    "    def get_soup(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"L·∫•y BeautifulSoup object t·ª´ URL\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                logger.info(f\"üîç Fetching: {url} (attempt {attempt + 1})\")\n",
    "                \n",
    "                response = self.session.get(url, timeout=self.timeout)\n",
    "                response.raise_for_status()\n",
    "                response.encoding = 'utf-8'\n",
    "                \n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Delay ƒë·ªÉ tr√°nh spam\n",
    "                time.sleep(self.delay)\n",
    "                \n",
    "                return soup\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ùå Error fetching {url} (attempt {attempt + 1}): {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                continue\n",
    "        \n",
    "        logger.error(f\"‚ùå Failed to fetch {url} after {retries} attempts\")\n",
    "        return None\n",
    "    \n",
    "    def safe_extract_text(self, soup: BeautifulSoup, selector: str, multiple: bool = False) -> Union[str, List[str]]:\n",
    "        \"\"\"Tr√≠ch xu·∫•t text an to√†n t·ª´ selector\"\"\"\n",
    "        try:\n",
    "            if multiple:\n",
    "                elements = soup.select(selector)\n",
    "                return [el.get_text(strip=True) for el in elements]\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                return element.get_text(strip=True) if element else \"\"\n",
    "        except Exception:\n",
    "            return [] if multiple else \"\"\n",
    "    \n",
    "    def extract_number(self, text: str) -> str:\n",
    "        \"\"\"Tr√≠ch xu·∫•t s·ªë t·ª´ text\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        cleaned = re.sub(r'[^\\d.,\\-]', '', text)\n",
    "        return cleaned\n",
    "    \n",
    "    def crawl_basic_info(self, symbol: str) -> Optional[StockBasicInfo]:\n",
    "        \"\"\"Crawl th√¥ng tin c∆° b·∫£n\"\"\"\n",
    "        url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"üîç Extracting basic info for {symbol.upper()}\")\n",
    "            \n",
    "            # T√™n c√¥ng ty t·ª´ h1\n",
    "            h1_element = soup.find('h1')\n",
    "            company_name = \"\"\n",
    "            if h1_element:\n",
    "                company_name = h1_element.get_text(strip=True)\n",
    "                if \"(\" in company_name:\n",
    "                    company_name = company_name.split(\"(\")[0].strip()\n",
    "            \n",
    "            # Gi√° hi·ªán t·∫°i v√† thay ƒë·ªïi\n",
    "            current_price = self.safe_extract_text(soup, \"#stockname_close\")\n",
    "            price_change = self.safe_extract_text(soup, \"#stockname_price_change\")\n",
    "            percent_change = self.safe_extract_text(soup, \"#stockname_percent_change\")\n",
    "            \n",
    "            # Th√¥ng tin giao d·ªãch\n",
    "            volume = self.safe_extract_text(soup, \"#stockname_volume\")\n",
    "            highest = self.safe_extract_text(soup, \"#stockname_price_highest\")\n",
    "            lowest = self.safe_extract_text(soup, \"#stockname_price_lowest\")\n",
    "            \n",
    "            # Th√¥ng tin chi ti·∫øt t·ª´ c√°c div\n",
    "            reference_price = \"\"\n",
    "            open_price = \"\"\n",
    "            \n",
    "            flex_detail_divs = soup.select(\".flex_detail\")\n",
    "            if len(flex_detail_divs) >= 2:\n",
    "                # Div ƒë·∫ßu c√≥ labels, div th·ª© 2 c√≥ values\n",
    "                value_div = flex_detail_divs[1]\n",
    "                value_elements = value_div.find_all('div')\n",
    "                if len(value_elements) >= 2:\n",
    "                    reference_price = value_elements[0].get_text(strip=True)\n",
    "                    open_price = value_elements[1].get_text(strip=True)\n",
    "            \n",
    "            return StockBasicInfo(\n",
    "                symbol=symbol.upper(),\n",
    "                company_name=company_name,\n",
    "                current_price=current_price,\n",
    "                price_change=price_change,\n",
    "                percent_change=percent_change,\n",
    "                reference_price=reference_price,\n",
    "                open_price=open_price,\n",
    "                high_price=highest,\n",
    "                low_price=lowest,\n",
    "                volume=volume,\n",
    "                timestamp=str(int(time.time()))\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting basic info for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_financial_ratios(self, symbol: str, soup: BeautifulSoup = None) -> Optional[StockFinancialRatios]:\n",
    "        \"\"\"Crawl c√°c ch·ªâ s·ªë t√†i ch√≠nh\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"üí∞ Extracting financial ratios for {symbol.upper()}\")\n",
    "            \n",
    "            ratios = StockFinancialRatios(symbol=symbol.upper())\n",
    "            \n",
    "            # T√¨m t·∫•t c·∫£ c√°c sections ch·ª©a d·ªØ li·ªáu t√†i ch√≠nh\n",
    "            flex_rows = soup.select(\".flex_row\")\n",
    "            \n",
    "            for row in flex_rows:\n",
    "                try:\n",
    "                    label_div = row.select_one(\".flex_detail\")\n",
    "                    value_div = row.select_one(\".flex_detail.bold\")\n",
    "                    \n",
    "                    if not label_div or not value_div:\n",
    "                        continue\n",
    "                    \n",
    "                    labels = [div.get_text(strip=True) for div in label_div.find_all('div')]\n",
    "                    values = [div.get_text(strip=True) for div in value_div.find_all('div')]\n",
    "                    \n",
    "                    # Map labels to values\n",
    "                    for i, label in enumerate(labels):\n",
    "                        if i < len(values):\n",
    "                            value = values[i]\n",
    "                            \n",
    "                            if \"Gi√° s·ªï s√°ch\" in label:\n",
    "                                ratios.book_value = value\n",
    "                            elif \"EPS\" in label and \"title\" not in label.lower():\n",
    "                                ratios.eps = value\n",
    "                            elif \"PE\" in label and \"title\" not in label.lower():\n",
    "                                ratios.pe_ratio = value\n",
    "                            elif \"PB\" in label and \"title\" not in label.lower():\n",
    "                                ratios.pb_ratio = value\n",
    "                            elif \"ROA\" in label:\n",
    "                                # ROA c√≥ th·ªÉ c√≥ format \"6% # 13%\"\n",
    "                                ratios.roa = value.split(\"#\")[0].strip()\n",
    "                            elif \"ROE\" in label:\n",
    "                                if \"#\" in value:\n",
    "                                    ratios.roe = value.split(\"#\")[1].strip()\n",
    "                                else:\n",
    "                                    ratios.roe = value\n",
    "                            elif \"Beta\" in label:\n",
    "                                ratios.beta = value\n",
    "                            elif \"V·ªën th·ªã tr∆∞·ªùng\" in label:\n",
    "                                ratios.market_cap = value\n",
    "                            elif \"KL ni√™m y·∫øt\" in label:\n",
    "                                ratios.listed_volume = value\n",
    "                            elif \"KLGD 52w\" in label:\n",
    "                                ratios.avg_volume_52w = value\n",
    "                            elif \"Cao - th·∫•p 52w\" in label:\n",
    "                                ratios.high_low_52w = value\n",
    "                                \n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            return ratios\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting financial ratios for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_balance_sheet(self, symbol: str, soup: BeautifulSoup = None) -> Optional[StockBalanceSheet]:\n",
    "        \"\"\"Crawl c·∫•u tr√∫c t√†i ch√≠nh\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"üìä Extracting balance sheet for {symbol.upper()}\")\n",
    "            \n",
    "            balance_sheet = StockBalanceSheet(symbol=symbol.upper())\n",
    "            \n",
    "            # T√¨m section c√≥ th√¥ng tin n·ª£/v·ªën\n",
    "            flex_rows = soup.select(\".flex_row\")\n",
    "            \n",
    "            for row in flex_rows:\n",
    "                try:\n",
    "                    row_text = row.get_text()\n",
    "                    if \"N·ª£\" in row_text and \"V·ªën CSH\" in row_text:\n",
    "                        label_div = row.select_one(\".flex_detail\")\n",
    "                        value_div = row.select_one(\".flex_detail.bold\")\n",
    "                        \n",
    "                        if not label_div or not value_div:\n",
    "                            continue\n",
    "                        \n",
    "                        labels = [div.get_text(strip=True) for div in label_div.find_all('div')]\n",
    "                        values = [div.get_text(strip=True) for div in value_div.find_all('div')]\n",
    "                        \n",
    "                        for i, label in enumerate(labels):\n",
    "                            if i < len(values):\n",
    "                                value = values[i]\n",
    "                                \n",
    "                                if label == \"N·ª£\":\n",
    "                                    balance_sheet.total_debt = value\n",
    "                                elif label == \"V·ªën CSH\":\n",
    "                                    balance_sheet.owner_equity = value\n",
    "                                elif \"%N·ª£/V·ªënCSH\" in label:\n",
    "                                    balance_sheet.debt_equity_ratio = value\n",
    "                                elif \"%V·ªën CSH/T√†iS·∫£n\" in label:\n",
    "                                    balance_sheet.equity_asset_ratio = value\n",
    "                                elif \"Ti·ªÅn m·∫∑t\" in label:\n",
    "                                    balance_sheet.cash = value\n",
    "                        break\n",
    "                        \n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            return balance_sheet\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting balance sheet for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_power_ratings(self, symbol: str, soup: BeautifulSoup = None) -> Optional[StockPowerRatings]:\n",
    "        \"\"\"Crawl s·ª©c m·∫°nh c√°c ch·ªâ s·ªë\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"‚ö° Extracting power ratings for {symbol.upper()}\")\n",
    "            \n",
    "            power_ratings = StockPowerRatings(symbol=symbol.upper())\n",
    "            \n",
    "            # T√¨m section c√≥ icon bolt (fa-bolt)\n",
    "            flex_rows = soup.select(\".flex_row\")\n",
    "            \n",
    "            for row in flex_rows:\n",
    "                if \"fa-bolt\" in str(row) or \"fa-solid fa-bolt\" in str(row):\n",
    "                    try:\n",
    "                        # Extract percentages t·ª´ text\n",
    "                        row_text = row.get_text()\n",
    "                        percentages = re.findall(r'(\\d+)%', row_text)\n",
    "                        \n",
    "                        if len(percentages) >= 5:\n",
    "                            power_ratings.eps_power = f\"{percentages[0]}%\"\n",
    "                            power_ratings.roe_power = f\"{percentages[1]}%\"\n",
    "                            power_ratings.pb_power = f\"{percentages[3]}%\"\n",
    "                            power_ratings.price_growth_power = f\"{percentages[4]}%\"\n",
    "                        \n",
    "                        # ƒê·∫∑c bi·ªát cho ƒë·∫ßu t∆∞ hi·ªáu qu·∫£ (rating sao)\n",
    "                        star_elements = row.select(\".fa-star\")\n",
    "                        if star_elements:\n",
    "                            # ƒê·∫øm s·ªë sao c√≥ m√†u xanh\n",
    "                            filled_stars = len([star for star in star_elements \n",
    "                                             if \"color: #006600\" in star.get('style', '')])\n",
    "                            power_ratings.investment_efficiency = f\"{filled_stars}/5 stars\"\n",
    "                        elif len(percentages) >= 3:\n",
    "                            power_ratings.investment_efficiency = f\"{percentages[2]}%\"\n",
    "                        \n",
    "                        break\n",
    "                        \n",
    "                    except Exception:\n",
    "                        continue\n",
    "            \n",
    "            return power_ratings\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting power ratings for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_trading_data(self, symbol: str, soup: BeautifulSoup = None) -> Optional[TradingData]:\n",
    "        \"\"\"Crawl d·ªØ li·ªáu giao d·ªãch\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"üìà Extracting trading data for {symbol.upper()}\")\n",
    "            \n",
    "            trading_data = TradingData(symbol=symbol.upper())\n",
    "            \n",
    "            # T√¨m b·∫£ng giao d·ªãch\n",
    "            tables = soup.find_all('table')\n",
    "            trading_table = None\n",
    "            \n",
    "            for table in tables:\n",
    "                table_text = table.get_text()\n",
    "                if \"MUA\" in table_text and \"B√ÅN\" in table_text:\n",
    "                    trading_table = table\n",
    "                    break\n",
    "            \n",
    "            if trading_table:\n",
    "                rows = trading_table.find_all('tr')\n",
    "                \n",
    "                # Skip header, l·∫•y 5 rows ƒë·∫ßu\n",
    "                for row in rows[1:6]:\n",
    "                    cells = row.find_all('td')\n",
    "                    if len(cells) >= 4:\n",
    "                        buy_price = cells[0].get_text(strip=True)\n",
    "                        buy_volume = cells[1].get_text(strip=True)\n",
    "                        sell_price = cells[2].get_text(strip=True)\n",
    "                        sell_volume = cells[3].get_text(strip=True)\n",
    "                        \n",
    "                        if buy_price and buy_volume:\n",
    "                            trading_data.buy_orders.append({\n",
    "                                \"price\": buy_price,\n",
    "                                \"volume\": buy_volume\n",
    "                            })\n",
    "                        \n",
    "                        if sell_price and sell_volume:\n",
    "                            trading_data.sell_orders.append({\n",
    "                                \"price\": sell_price,\n",
    "                                \"volume\": sell_volume\n",
    "                            })\n",
    "                \n",
    "                # Th√¥ng tin n∆∞·ªõc ngo√†i\n",
    "                foreign_buy_element = soup.select_one(\"#foreigner_buy_volume\")\n",
    "                foreign_sell_element = soup.select_one(\"#foreigner_sell_volume\")\n",
    "                \n",
    "                if foreign_buy_element:\n",
    "                    trading_data.foreign_buy = foreign_buy_element.get_text(strip=True)\n",
    "                if foreign_sell_element:\n",
    "                    trading_data.foreign_sell = foreign_sell_element.get_text(strip=True)\n",
    "            \n",
    "            return trading_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting trading data for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_financial_statements(self, symbol: str, soup: BeautifulSoup = None) -> List[FinancialStatement]:\n",
    "        \"\"\"Crawl b√°o c√°o t√†i ch√≠nh\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"üìã Extracting financial statements for {symbol.upper()}\")\n",
    "            \n",
    "            statements = []\n",
    "            \n",
    "            # T√¨m b·∫£ng b√°o c√°o t√†i ch√≠nh\n",
    "            financial_table = soup.find('table', {'id': 'financial_brief'})\n",
    "            \n",
    "            if not financial_table:\n",
    "                return []\n",
    "            \n",
    "            rows = financial_table.find_all('tr')\n",
    "            if len(rows) < 2:\n",
    "                return []\n",
    "            \n",
    "            # Header row - c√°c periods\n",
    "            header_row = rows[0]\n",
    "            period_cells = header_row.find_all('td')[1:]  # Skip first cell\n",
    "            periods = [cell.get_text(strip=True) for cell in period_cells]\n",
    "            \n",
    "            # Data rows\n",
    "            data_rows = {}\n",
    "            for row in rows[1:]:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) > 1:\n",
    "                    indicator = cells[0].get_text(strip=True)\n",
    "                    values = [cell.get_text(strip=True) for cell in cells[1:]]\n",
    "                    data_rows[indicator] = values\n",
    "            \n",
    "            # T·∫°o FinancialStatement cho m·ªói period\n",
    "            for i, period in enumerate(periods):\n",
    "                statement = FinancialStatement(\n",
    "                    symbol=symbol.upper(),\n",
    "                    period=period\n",
    "                )\n",
    "                \n",
    "                # Map indicators to fields\n",
    "                for indicator, values in data_rows.items():\n",
    "                    if i < len(values):\n",
    "                        value = values[i]\n",
    "                        \n",
    "                        if \"Doanh thu\" in indicator:\n",
    "                            statement.revenue = value\n",
    "                        elif \"T·ªïng l·ª£i nhu·∫≠n tr∆∞·ªõc thu·∫ø\" in indicator:\n",
    "                            statement.profit_before_tax = value\n",
    "                        elif \"L·ª£i nhu·∫≠n sau thu·∫ø\" in indicator and \"c√¥ng ty m·∫π\" not in indicator:\n",
    "                            statement.net_profit = value\n",
    "                        elif \"L·ª£i nhu·∫≠n sau thu·∫ø c·ªßa c√¥ng ty m·∫π\" in indicator:\n",
    "                            statement.parent_profit = value\n",
    "                        elif \"T·ªïng t√†i s·∫£n\" in indicator:\n",
    "                            statement.total_assets = value\n",
    "                        elif \"T·ªïng n·ª£\" in indicator:\n",
    "                            statement.total_debt = value\n",
    "                        elif \"V·ªën ch·ªß s·ªü h·ªØu\" in indicator:\n",
    "                            statement.owner_equity = value\n",
    "                \n",
    "                statements.append(statement)\n",
    "            \n",
    "            return statements\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting financial statements for {symbol}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def crawl_business_plan(self, symbol: str, soup: BeautifulSoup = None) -> List[BusinessPlan]:\n",
    "        \"\"\"Crawl k·∫ø ho·∫°ch kinh doanh\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"üìÖ Extracting business plan for {symbol.upper()}\")\n",
    "            \n",
    "            plans = []\n",
    "            \n",
    "            # T√¨m section k·∫ø ho·∫°ch kinh doanh\n",
    "            business_plan_div = soup.find('div', {'id': 'business_plan'})\n",
    "            \n",
    "            if not business_plan_div:\n",
    "                return []\n",
    "            \n",
    "            table = business_plan_div.find('table')\n",
    "            if not table:\n",
    "                return []\n",
    "            \n",
    "            rows = table.find_all('tr')[1:]  # Skip header\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 5:\n",
    "                    year = cells[0].get_text(strip=True)\n",
    "                    revenue_plan = cells[1].get_text(strip=True)\n",
    "                    revenue_achievement = cells[2].get_text(strip=True)\n",
    "                    profit_plan = cells[3].get_text(strip=True)\n",
    "                    profit_achievement = cells[4].get_text(strip=True)\n",
    "                    \n",
    "                    plan = BusinessPlan(\n",
    "                        symbol=symbol.upper(),\n",
    "                        year=year,\n",
    "                        revenue_plan=revenue_plan,\n",
    "                        revenue_achievement=revenue_achievement,\n",
    "                        profit_plan=profit_plan,\n",
    "                        profit_achievement=profit_achievement\n",
    "                    )\n",
    "                    plans.append(plan)\n",
    "            \n",
    "            return plans\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting business plan for {symbol}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def crawl_industry_info(self, symbol: str, soup: BeautifulSoup = None) -> Optional[IndustryInfo]:\n",
    "        \"\"\"Crawl th√¥ng tin ng√†nh\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"üè≠ Extracting industry info for {symbol.upper()}\")\n",
    "            \n",
    "            industry_info = IndustryInfo(symbol=symbol.upper())\n",
    "            \n",
    "            # T√¨m th√¥ng tin ng√†nh t·ª´ h2 \"Ng√†nh/Nh√≥m/H·ªç\"\n",
    "            h2_elements = soup.find_all('h2')\n",
    "            \n",
    "            for h2 in h2_elements:\n",
    "                if \"Ng√†nh/Nh√≥m/H·ªç\" in h2.get_text():\n",
    "                    # T√¨m table ngay sau h2\n",
    "                    next_sibling = h2.find_next_sibling()\n",
    "                    if next_sibling:\n",
    "                        table = next_sibling.find('table')\n",
    "                        if table:\n",
    "                            cell = table.find('td')\n",
    "                            if cell:\n",
    "                                text = cell.get_text(strip=True)\n",
    "                                lines = text.split('\\n')\n",
    "                                \n",
    "                                if len(lines) >= 1:\n",
    "                                    industry_info.market_name = lines[0].strip()\n",
    "                                if len(lines) >= 2:\n",
    "                                    # Lo·∫°i b·ªè ph·∫ßn trong ngo·∫∑c\n",
    "                                    industry_name = lines[1].strip()\n",
    "                                    if '(' in industry_name:\n",
    "                                        industry_name = industry_name.split('(')[0].strip()\n",
    "                                    industry_info.industry_name = industry_name\n",
    "                    break\n",
    "            \n",
    "            return industry_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting industry info for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_company_profile(self, symbol: str) -> Optional[CompanyProfile]:\n",
    "        \"\"\"Crawl th√¥ng tin chi ti·∫øt c√¥ng ty t·ª´ trang profile\"\"\"\n",
    "        url = f\"{self.urls['profile']}{symbol.upper()}\"\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"üè¢ Extracting company profile for {symbol.upper()}\")\n",
    "            \n",
    "            profile = CompanyProfile(symbol=symbol.upper())\n",
    "            \n",
    "            # T√¨m c√°c th√¥ng tin trong table ho·∫∑c div ch·ª©a th√¥ng tin c√¥ng ty\n",
    "            # C·∫•u tr√∫c c√≥ th·ªÉ thay ƒë·ªïi, c·∫ßn flexible parsing\n",
    "            \n",
    "            tables = soup.find_all('table')\n",
    "            for table in tables:\n",
    "                rows = table.find_all('tr')\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    if len(cells) >= 2:\n",
    "                        label = cells[0].get_text(strip=True).lower()\n",
    "                        value = cells[1].get_text(strip=True)\n",
    "                        \n",
    "                        if \"t√™n ƒë·∫ßy ƒë·ªß\" in label or \"t√™n c√¥ng ty\" in label:\n",
    "                            profile.full_name = value\n",
    "                        elif \"t√™n ti·∫øng anh\" in label:\n",
    "                            profile.english_name = value\n",
    "                        elif \"t√™n vi·∫øt t·∫Øt\" in label:\n",
    "                            profile.short_name = value\n",
    "                        elif \"ƒë·ªãa ch·ªâ\" in label:\n",
    "                            profile.address = value\n",
    "                        elif \"ƒëi·ªán tho·∫°i\" in label:\n",
    "                            profile.phone = value\n",
    "                        elif \"fax\" in label:\n",
    "                            profile.fax = value\n",
    "                        elif \"website\" in label:\n",
    "                            profile.website = value\n",
    "                        elif \"email\" in label:\n",
    "                            profile.email = value\n",
    "                        elif \"ng√†y th√†nh l·∫≠p\" in label:\n",
    "                            profile.established_date = value\n",
    "                        elif \"ng√†y ni√™m y·∫øt\" in label:\n",
    "                            profile.listed_date = value\n",
    "                        elif \"v·ªën ƒëi·ªÅu l·ªá\" in label:\n",
    "                            profile.chartered_capital = value\n",
    "                        elif \"gi·∫•y ph√©p kinh doanh\" in label:\n",
    "                            profile.business_license = value\n",
    "                        elif \"m√£ s·ªë thu·∫ø\" in label:\n",
    "                            profile.tax_code = value\n",
    "            \n",
    "            return profile\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting company profile for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_complete_stock_data(self, symbol: str) -> CompleteStockData:\n",
    "        \"\"\"Crawl t·∫•t c·∫£ d·ªØ li·ªáu c·ªßa m·ªôt c·ªï phi·∫øu\"\"\"\n",
    "        logger.info(f\"üéØ Starting complete crawl for {symbol.upper()}\")\n",
    "        \n",
    "        # L·∫•y soup t·ª´ trang summary m·ªôt l·∫ßn ƒë·ªÉ t√°i s·ª≠ d·ª•ng\n",
    "        summary_url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "        soup = self.get_soup(summary_url)\n",
    "        \n",
    "        complete_data = CompleteStockData()\n",
    "        \n",
    "        if soup:\n",
    "            # Crawl t·∫•t c·∫£ d·ªØ li·ªáu t·ª´ trang summary\n",
    "            complete_data.basic_info = self.crawl_basic_info(symbol)\n",
    "            complete_data.financial_ratios = self.crawl_financial_ratios(symbol, soup)\n",
    "            complete_data.balance_sheet = self.crawl_balance_sheet(symbol, soup)\n",
    "            complete_data.power_ratings = self.crawl_power_ratings(symbol, soup)\n",
    "            complete_data.trading_data = self.crawl_trading_data(symbol, soup)\n",
    "            complete_data.financial_statements = self.crawl_financial_statements(symbol, soup)\n",
    "            complete_data.business_plans = self.crawl_business_plan(symbol, soup)\n",
    "            complete_data.industry_info = self.crawl_industry_info(symbol, soup)\n",
    "        \n",
    "        # Crawl company profile t·ª´ trang ri√™ng\n",
    "        complete_data.company_profile = self.crawl_company_profile(symbol)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Completed crawl for {symbol.upper()}\")\n",
    "        return complete_data\n",
    "    \n",
    "    def crawl_multiple_stocks(self, symbols: List[str], max_workers: int = 5) -> Dict[str, CompleteStockData]:\n",
    "        \"\"\"Crawl nhi·ªÅu c·ªï phi·∫øu song song\"\"\"\n",
    "        logger.info(f\"üöÄ Starting batch crawl for {len(symbols)} symbols\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ crawl song song\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit t·∫•t c·∫£ tasks\n",
    "            future_to_symbol = {\n",
    "                executor.submit(self.crawl_complete_stock_data, symbol): symbol \n",
    "                for symbol in symbols\n",
    "            }\n",
    "            \n",
    "            # Collect results\n",
    "            for future in concurrent.futures.as_completed(future_to_symbol):\n",
    "                symbol = future_to_symbol[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    results[symbol.upper()] = data\n",
    "                    logger.info(f\"‚úÖ Completed {symbol.upper()}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"‚ùå Error crawling {symbol}: {e}\")\n",
    "                    results[symbol.upper()] = CompleteStockData()\n",
    "        \n",
    "        logger.info(f\"üéâ Batch crawl completed: {len(results)} symbols processed\")\n",
    "        return results\n",
    "    \n",
    "    def crawl_market_list(self, market_type: str = \"all\") -> List[str]:\n",
    "        \"\"\"Crawl danh s√°ch m√£ c·ªï phi·∫øu t·ª´ th·ªã tr∆∞·ªùng\"\"\"\n",
    "        url = f\"{self.urls['market_data']}?market={market_type}\"\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"üìã Extracting stock list from market: {market_type}\")\n",
    "            \n",
    "            symbols = []\n",
    "            \n",
    "            # T√¨m links c√≥ pattern /quote/summary.php?id=\n",
    "            links = soup.find_all('a', href=re.compile(r'/quote/summary\\.php\\?id='))\n",
    "            \n",
    "            for link in links:\n",
    "                href = link.get('href', '')\n",
    "                match = re.search(r'id=([A-Z0-9]+)', href)\n",
    "                if match:\n",
    "                    symbol = match.group(1).upper()\n",
    "                    if symbol not in symbols:\n",
    "                        symbols.append(symbol)\n",
    "            \n",
    "            logger.info(f\"‚úÖ Found {len(symbols)} symbols in {market_type} market\")\n",
    "            return symbols\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting market list: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def crawl_industry_list(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Crawl danh s√°ch ng√†nh\"\"\"\n",
    "        url = self.urls['categories']\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"üè≠ Extracting industry list\")\n",
    "            \n",
    "            industries = []\n",
    "            \n",
    "            # T√¨m c√°c links ng√†nh\n",
    "            links = soup.find_all('a', href=re.compile(r'category'))\n",
    "            \n",
    "            for link in links[:20]:  # Gi·ªõi h·∫°n 20 ng√†nh ƒë·∫ßu\n",
    "                try:\n",
    "                    industry_name = link.get_text(strip=True)\n",
    "                    industry_url = link.get('href', '')\n",
    "                    \n",
    "                    if industry_name and industry_url:\n",
    "                        # ƒê·∫£m b·∫£o URL ƒë·∫ßy ƒë·ªß\n",
    "                        if not industry_url.startswith('http'):\n",
    "                            industry_url = urljoin(self.urls['base_url'], industry_url)\n",
    "                        \n",
    "                        industries.append({\n",
    "                            \"name\": industry_name,\n",
    "                            \"url\": industry_url\n",
    "                        })\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"‚úÖ Found {len(industries)} industries\")\n",
    "            return industries\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting industry list: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def save_results(self, data: Dict[str, CompleteStockData], output_dir: str = \"results\"):\n",
    "        \"\"\"L∆∞u k·∫øt qu·∫£ ra files\"\"\"\n",
    "        import os\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # L∆∞u t·ª´ng lo·∫°i d·ªØ li·ªáu ri√™ng\n",
    "        basic_info_list = []\n",
    "        financial_ratios_list = []\n",
    "        balance_sheet_list = []\n",
    "        power_ratings_list = []\n",
    "        trading_data_list = []\n",
    "        financial_statements_list = []\n",
    "        business_plans_list = []\n",
    "        industry_info_list = []\n",
    "        company_profiles_list = []\n",
    "        \n",
    "        for symbol, stock_data in data.items():\n",
    "            if stock_data.basic_info:\n",
    "                basic_info_list.append(asdict(stock_data.basic_info))\n",
    "            \n",
    "            if stock_data.financial_ratios:\n",
    "                financial_ratios_list.append(asdict(stock_data.financial_ratios))\n",
    "            \n",
    "            if stock_data.balance_sheet:\n",
    "                balance_sheet_list.append(asdict(stock_data.balance_sheet))\n",
    "            \n",
    "            if stock_data.power_ratings:\n",
    "                power_ratings_list.append(asdict(stock_data.power_ratings))\n",
    "            \n",
    "            if stock_data.trading_data:\n",
    "                trading_data_list.append(asdict(stock_data.trading_data))\n",
    "            \n",
    "            if stock_data.financial_statements:\n",
    "                for stmt in stock_data.financial_statements:\n",
    "                    financial_statements_list.append(asdict(stmt))\n",
    "            \n",
    "            if stock_data.business_plans:\n",
    "                for plan in stock_data.business_plans:\n",
    "                    business_plans_list.append(asdict(plan))\n",
    "            \n",
    "            if stock_data.industry_info:\n",
    "                industry_info_list.append(asdict(stock_data.industry_info))\n",
    "            \n",
    "            if stock_data.company_profile:\n",
    "                company_profiles_list.append(asdict(stock_data.company_profile))\n",
    "        \n",
    "        # L∆∞u ra CSV files\n",
    "        datasets = {\n",
    "            \"basic_info\": basic_info_list,\n",
    "            \"financial_ratios\": financial_ratios_list,\n",
    "            \"balance_sheet\": balance_sheet_list,\n",
    "            \"power_ratings\": power_ratings_list,\n",
    "            \"trading_data\": trading_data_list,\n",
    "            \"financial_statements\": financial_statements_list,\n",
    "            \"business_plans\": business_plans_list,\n",
    "            \"industry_info\": industry_info_list,\n",
    "            \"company_profiles\": company_profiles_list\n",
    "        }\n",
    "        \n",
    "        for name, dataset in datasets.items():\n",
    "            if dataset:\n",
    "                df = pd.DataFrame(dataset)\n",
    "                csv_path = os.path.join(output_dir, f\"{name}.csv\")\n",
    "                df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "                logger.info(f\"üíæ Saved {len(dataset)} records to {csv_path}\")\n",
    "        \n",
    "        # L∆∞u raw data d·∫°ng JSON\n",
    "        json_data = {}\n",
    "        for symbol, stock_data in data.items():\n",
    "            json_data[symbol] = {\n",
    "                \"basic_info\": asdict(stock_data.basic_info) if stock_data.basic_info else None,\n",
    "                \"financial_ratios\": asdict(stock_data.financial_ratios) if stock_data.financial_ratios else None,\n",
    "                \"balance_sheet\": asdict(stock_data.balance_sheet) if stock_data.balance_sheet else None,\n",
    "                \"power_ratings\": asdict(stock_data.power_ratings) if stock_data.power_ratings else None,\n",
    "                \"trading_data\": asdict(stock_data.trading_data) if stock_data.trading_data else None,\n",
    "                \"financial_statements\": [asdict(stmt) for stmt in stock_data.financial_statements],\n",
    "                \"business_plans\": [asdict(plan) for plan in stock_data.business_plans],\n",
    "                \"industry_info\": asdict(stock_data.industry_info) if stock_data.industry_info else None,\n",
    "                \"company_profile\": asdict(stock_data.company_profile) if stock_data.company_profile else None\n",
    "            }\n",
    "        \n",
    "        json_path = os.path.join(output_dir, \"complete_data.json\")\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(f\"üíæ Saved complete data to {json_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"H√†m ch√≠nh ƒë·ªÉ test crawler\"\"\"\n",
    "    print(\"üöÄ Cophieu68 BeautifulSoup Crawler\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Kh·ªüi t·∫°o crawler\n",
    "    crawler = Cophieu68BeautifulSoupCrawler(delay=1.0)\n",
    "    \n",
    "    # Test symbols\n",
    "    test_symbols = [\"VCB\", \"VIC\", \"VHM\", \"HPG\", \"MSN\", \"DNN\"]\n",
    "    \n",
    "    try:\n",
    "        # Test crawl m·ªôt symbol ƒë·∫ßy ƒë·ªß\n",
    "        print(f\"\\nüéØ Test complete crawl for {test_symbols[0]}\")\n",
    "        complete_data = crawler.crawl_complete_stock_data(test_symbols[0])\n",
    "        \n",
    "        # In th√¥ng tin c∆° b·∫£n\n",
    "        if complete_data.basic_info:\n",
    "            print(f\"  ‚úÖ Basic Info: {complete_data.basic_info.company_name}\")\n",
    "            print(f\"     Price: {complete_data.basic_info.current_price}\")\n",
    "            print(f\"     Change: {complete_data.basic_info.price_change} ({complete_data.basic_info.percent_change})\")\n",
    "        \n",
    "        if complete_data.financial_ratios:\n",
    "            print(f\"  ‚úÖ Financial Ratios: PE={complete_data.financial_ratios.pe_ratio}, PB={complete_data.financial_ratios.pb_ratio}\")\n",
    "        \n",
    "        if complete_data.financial_statements:\n",
    "            print(f\"  ‚úÖ Financial Statements: {len(complete_data.financial_statements)} periods\")\n",
    "        \n",
    "        if complete_data.industry_info:\n",
    "            print(f\"  ‚úÖ Industry: {complete_data.industry_info.market_name} - {complete_data.industry_info.industry_name}\")\n",
    "        \n",
    "        # Test batch crawl\n",
    "        print(f\"\\nüöÄ Test batch crawl for {len(test_symbols[:3])} symbols\")\n",
    "        batch_results = crawler.crawl_multiple_stocks(test_symbols[:3], max_workers=3)\n",
    "        \n",
    "        for symbol, data in batch_results.items():\n",
    "            status = \"‚úÖ\" if data.basic_info else \"‚ùå\"\n",
    "            company_name = data.basic_info.company_name if data.basic_info else \"N/A\"\n",
    "            print(f\"  {status} {symbol}: {company_name}\")\n",
    "        \n",
    "        # Test crawl market list\n",
    "        print(f\"\\nüìã Test market list crawl\")\n",
    "        market_symbols = crawler.crawl_market_list(\"vnall\")\n",
    "        print(f\"  ‚úÖ Found {len(market_symbols)} symbols in market\")\n",
    "        if market_symbols:\n",
    "            print(f\"  üìù Sample: {market_symbols[:10]}\")\n",
    "        \n",
    "        # Test crawl industry list\n",
    "        print(f\"\\nüè≠ Test industry list crawl\")\n",
    "        industries = crawler.crawl_industry_list()\n",
    "        print(f\"  ‚úÖ Found {len(industries)} industries\")\n",
    "        for industry in industries[:5]:\n",
    "            print(f\"    - {industry.get('name', 'N/A')}\")\n",
    "        \n",
    "        # L∆∞u k·∫øt qu·∫£\n",
    "        print(f\"\\nüíæ Saving results...\")\n",
    "        crawler.save_results(batch_results, \"beautifulsoup_results\")\n",
    "        \n",
    "        print(f\"\\nüéâ Test completed successfully!\")\n",
    "        print(f\"üìÅ Check results in 'beautifulsoup_results/' folder\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error in main: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ee7e9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
