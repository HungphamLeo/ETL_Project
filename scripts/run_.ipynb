{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ae313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# PROJECT_ROOT = os.path.dirname(os.path.abspath(\"\"))\n",
    "\n",
    "# if PROJECT_ROOT not in sys.path:\n",
    "#     sys.path.insert(0, PROJECT_ROOT)\n",
    "# from scripts.extract import WorldBankExtractOperator\n",
    "# from scripts.transform import SparkTransformOperator\n",
    "# from scripts.loads import DatabaseLoaderService\n",
    "# from scripts.load_wbapi_etl_config import ETLPipelineConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b5c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kha thi\n",
      "<Response [200]>\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\python_enviroment\\Lib\\site-packages\\requests\\models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\python_enviroment\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\python_enviroment\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\python_enviroment\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mfetch_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkha thi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\python_enviroment\\Lib\\site-packages\\requests\\models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cophieu68 Crawler với BeautifulSoup\n",
    "Nhanh hơn, ổn định hơn, ít tài nguyên hơn Selenium\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, Dict, List, Any, Union\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Cophieu68BeautifulSoupCrawler:\n",
    "    \"\"\"Crawler sử dụng BeautifulSoup cho cophieu68.vn\"\"\"\n",
    "    \n",
    "    def __init__(self, delay: float = 1.0, timeout: int = 30):\n",
    "        self.delay = delay\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Setup headers để tránh bị block\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        \n",
    "        # URLs mapping\n",
    "        self.urls = {\n",
    "            \"base_url\": \"https://www.cophieu68.vn\",\n",
    "            \"summary\": \"https://www.cophieu68.vn/quote/summary.php?id=\",\n",
    "            \"financial\": \"https://www.cophieu68.vn/quote/financial.php?id=\",\n",
    "            \"financial_detail\": \"https://www.cophieu68.vn/quote/financial_detail.php?id=\",\n",
    "            \"profile\": \"https://www.cophieu68.vn/quote/profile.php?id=\",\n",
    "            \"history\": \"https://www.cophieu68.vn/quote/history.php?id=\",\n",
    "            \"events\": \"https://www.cophieu68.vn/quote/event.php?id=\",\n",
    "            \"chart\": \"https://www.cophieu68.vn/chart/chart.php?id=\",\n",
    "            \"market_data\": \"https://www.cophieu68.vn/market/markets.php\",\n",
    "            \"categories\": \"https://www.cophieu68.vn/category/category_index.php\",\n",
    "            \"foreigner\": \"https://www.cophieu68.vn/stats/foreigner.php\",\n",
    "            \"volume_buzz\": \"https://www.cophieu68.vn/stats/volume_buzz.php\",\n",
    "            \"stats\": \"https://www.cophieu68.vn/stats/stats.php\",\n",
    "        }\n",
    "        \n",
    "        logger.info(\"✅ Cophieu68 BeautifulSoup Crawler initialized\")\n",
    "    \n",
    "    def get_soup(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Lấy BeautifulSoup object từ URL\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                logger.info(f\"🔍 Fetching: {url} (attempt {attempt + 1})\")\n",
    "                \n",
    "                response = self.session.get(url, timeout=self.timeout)\n",
    "                response.raise_for_status()\n",
    "                response.encoding = 'utf-8'\n",
    "                \n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Delay để tránh spam\n",
    "                time.sleep(self.delay)\n",
    "                \n",
    "                return soup\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"❌ Error fetching {url} (attempt {attempt + 1}): {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                continue\n",
    "        \n",
    "        logger.error(f\"❌ Failed to fetch {url} after {retries} attempts\")\n",
    "        return None\n",
    "    \n",
    "    def safe_extract_text(self, soup: BeautifulSoup, selector: str, multiple: bool = False) -> Union[str, List[str]]:\n",
    "        \"\"\"Trích xuất text an toàn từ selector\"\"\"\n",
    "        try:\n",
    "            if multiple:\n",
    "                elements = soup.select(selector)\n",
    "                return [el.get_text(strip=True) for el in elements]\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                return element.get_text(strip=True) if element else \"\"\n",
    "        except Exception:\n",
    "            return [] if multiple else \"\"\n",
    "    \n",
    "    def extract_number(self, text: str) -> str:\n",
    "        \"\"\"Trích xuất số từ text\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        cleaned = re.sub(r'[^\\d.,\\-]', '', text)\n",
    "        return cleaned\n",
    "    \n",
    "    def crawl_basic_info(self, symbol: str) -> Optional[StockBasicInfo]:\n",
    "        \"\"\"Crawl thông tin cơ bản\"\"\"\n",
    "        url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"🔍 Extracting basic info for {symbol.upper()}\")\n",
    "            \n",
    "            # Tên công ty từ h1\n",
    "            h1_element = soup.find('h1')\n",
    "            company_name = \"\"\n",
    "            if h1_element:\n",
    "                company_name = h1_element.get_text(strip=True)\n",
    "                if \"(\" in company_name:\n",
    "                    company_name = company_name.split(\"(\")[0].strip()\n",
    "            \n",
    "            # Giá hiện tại và thay đổi\n",
    "            current_price = self.safe_extract_text(soup, \"#stockname_close\")\n",
    "            price_change = self.safe_extract_text(soup, \"#stockname_price_change\")\n",
    "            percent_change = self.safe_extract_text(soup, \"#stockname_percent_change\")\n",
    "            \n",
    "            # Thông tin giao dịch\n",
    "            volume = self.safe_extract_text(soup, \"#stockname_volume\")\n",
    "            highest = self.safe_extract_text(soup, \"#stockname_price_highest\")\n",
    "            lowest = self.safe_extract_text(soup, \"#stockname_price_lowest\")\n",
    "            \n",
    "            # Thông tin chi tiết từ các div\n",
    "            reference_price = \"\"\n",
    "            open_price = \"\"\n",
    "            \n",
    "            flex_detail_divs = soup.select(\".flex_detail\")\n",
    "            if len(flex_detail_divs) >= 2:\n",
    "                # Div đầu có labels, div thứ 2 có values\n",
    "                value_div = flex_detail_divs[1]\n",
    "                value_elements = value_div.find_all('div')\n",
    "                if len(value_elements) >= 2:\n",
    "                    reference_price = value_elements[0].get_text(strip=True)\n",
    "                    open_price = value_elements[1].get_text(strip=True)\n",
    "            \n",
    "            return StockBasicInfo(\n",
    "                symbol=symbol.upper(),\n",
    "                company_name=company_name,\n",
    "                current_price=current_price,\n",
    "                price_change=price_change,\n",
    "                percent_change=percent_change,\n",
    "                reference_price=reference_price,\n",
    "                open_price=open_price,\n",
    "                high_price=highest,\n",
    "                low_price=lowest,\n",
    "                volume=volume,\n",
    "                timestamp=str(int(time.time()))\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting basic info for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_financial_ratios(self, symbol: str, soup: BeautifulSoup = None) -> Optional[StockFinancialRatios]:\n",
    "        \"\"\"Crawl các chỉ số tài chính\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"💰 Extracting financial ratios for {symbol.upper()}\")\n",
    "            \n",
    "            ratios = StockFinancialRatios(symbol=symbol.upper())\n",
    "            \n",
    "            # Tìm tất cả các sections chứa dữ liệu tài chính\n",
    "            flex_rows = soup.select(\".flex_row\")\n",
    "            \n",
    "            for row in flex_rows:\n",
    "                try:\n",
    "                    label_div = row.select_one(\".flex_detail\")\n",
    "                    value_div = row.select_one(\".flex_detail.bold\")\n",
    "                    \n",
    "                    if not label_div or not value_div:\n",
    "                        continue\n",
    "                    \n",
    "                    labels = [div.get_text(strip=True) for div in label_div.find_all('div')]\n",
    "                    values = [div.get_text(strip=True) for div in value_div.find_all('div')]\n",
    "                    \n",
    "                    # Map labels to values\n",
    "                    for i, label in enumerate(labels):\n",
    "                        if i < len(values):\n",
    "                            value = values[i]\n",
    "                            \n",
    "                            if \"Giá sổ sách\" in label:\n",
    "                                ratios.book_value = value\n",
    "                            elif \"EPS\" in label and \"title\" not in label.lower():\n",
    "                                ratios.eps = value\n",
    "                            elif \"PE\" in label and \"title\" not in label.lower():\n",
    "                                ratios.pe_ratio = value\n",
    "                            elif \"PB\" in label and \"title\" not in label.lower():\n",
    "                                ratios.pb_ratio = value\n",
    "                            elif \"ROA\" in label:\n",
    "                                # ROA có thể có format \"6% # 13%\"\n",
    "                                ratios.roa = value.split(\"#\")[0].strip()\n",
    "                            elif \"ROE\" in label:\n",
    "                                if \"#\" in value:\n",
    "                                    ratios.roe = value.split(\"#\")[1].strip()\n",
    "                                else:\n",
    "                                    ratios.roe = value\n",
    "                            elif \"Beta\" in label:\n",
    "                                ratios.beta = value\n",
    "                            elif \"Vốn thị trường\" in label:\n",
    "                                ratios.market_cap = value\n",
    "                            elif \"KL niêm yết\" in label:\n",
    "                                ratios.listed_volume = value\n",
    "                            elif \"KLGD 52w\" in label:\n",
    "                                ratios.avg_volume_52w = value\n",
    "                            elif \"Cao - thấp 52w\" in label:\n",
    "                                ratios.high_low_52w = value\n",
    "                                \n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            return ratios\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting financial ratios for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_balance_sheet(self, symbol: str, soup: BeautifulSoup = None) -> Optional[StockBalanceSheet]:\n",
    "        \"\"\"Crawl cấu trúc tài chính\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"📊 Extracting balance sheet for {symbol.upper()}\")\n",
    "            \n",
    "            balance_sheet = StockBalanceSheet(symbol=symbol.upper())\n",
    "            \n",
    "            # Tìm section có thông tin nợ/vốn\n",
    "            flex_rows = soup.select(\".flex_row\")\n",
    "            \n",
    "            for row in flex_rows:\n",
    "                try:\n",
    "                    row_text = row.get_text()\n",
    "                    if \"Nợ\" in row_text and \"Vốn CSH\" in row_text:\n",
    "                        label_div = row.select_one(\".flex_detail\")\n",
    "                        value_div = row.select_one(\".flex_detail.bold\")\n",
    "                        \n",
    "                        if not label_div or not value_div:\n",
    "                            continue\n",
    "                        \n",
    "                        labels = [div.get_text(strip=True) for div in label_div.find_all('div')]\n",
    "                        values = [div.get_text(strip=True) for div in value_div.find_all('div')]\n",
    "                        \n",
    "                        for i, label in enumerate(labels):\n",
    "                            if i < len(values):\n",
    "                                value = values[i]\n",
    "                                \n",
    "                                if label == \"Nợ\":\n",
    "                                    balance_sheet.total_debt = value\n",
    "                                elif label == \"Vốn CSH\":\n",
    "                                    balance_sheet.owner_equity = value\n",
    "                                elif \"%Nợ/VốnCSH\" in label:\n",
    "                                    balance_sheet.debt_equity_ratio = value\n",
    "                                elif \"%Vốn CSH/TàiSản\" in label:\n",
    "                                    balance_sheet.equity_asset_ratio = value\n",
    "                                elif \"Tiền mặt\" in label:\n",
    "                                    balance_sheet.cash = value\n",
    "                        break\n",
    "                        \n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            return balance_sheet\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting balance sheet for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_power_ratings(self, symbol: str, soup: BeautifulSoup = None) -> Optional[StockPowerRatings]:\n",
    "        \"\"\"Crawl sức mạnh các chỉ số\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"⚡ Extracting power ratings for {symbol.upper()}\")\n",
    "            \n",
    "            power_ratings = StockPowerRatings(symbol=symbol.upper())\n",
    "            \n",
    "            # Tìm section có icon bolt (fa-bolt)\n",
    "            flex_rows = soup.select(\".flex_row\")\n",
    "            \n",
    "            for row in flex_rows:\n",
    "                if \"fa-bolt\" in str(row) or \"fa-solid fa-bolt\" in str(row):\n",
    "                    try:\n",
    "                        # Extract percentages từ text\n",
    "                        row_text = row.get_text()\n",
    "                        percentages = re.findall(r'(\\d+)%', row_text)\n",
    "                        \n",
    "                        if len(percentages) >= 5:\n",
    "                            power_ratings.eps_power = f\"{percentages[0]}%\"\n",
    "                            power_ratings.roe_power = f\"{percentages[1]}%\"\n",
    "                            power_ratings.pb_power = f\"{percentages[3]}%\"\n",
    "                            power_ratings.price_growth_power = f\"{percentages[4]}%\"\n",
    "                        \n",
    "                        # Đặc biệt cho đầu tư hiệu quả (rating sao)\n",
    "                        star_elements = row.select(\".fa-star\")\n",
    "                        if star_elements:\n",
    "                            # Đếm số sao có màu xanh\n",
    "                            filled_stars = len([star for star in star_elements \n",
    "                                             if \"color: #006600\" in star.get('style', '')])\n",
    "                            power_ratings.investment_efficiency = f\"{filled_stars}/5 stars\"\n",
    "                        elif len(percentages) >= 3:\n",
    "                            power_ratings.investment_efficiency = f\"{percentages[2]}%\"\n",
    "                        \n",
    "                        break\n",
    "                        \n",
    "                    except Exception:\n",
    "                        continue\n",
    "            \n",
    "            return power_ratings\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting power ratings for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_trading_data(self, symbol: str, soup: BeautifulSoup = None) -> Optional[TradingData]:\n",
    "        \"\"\"Crawl dữ liệu giao dịch\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"📈 Extracting trading data for {symbol.upper()}\")\n",
    "            \n",
    "            trading_data = TradingData(symbol=symbol.upper())\n",
    "            \n",
    "            # Tìm bảng giao dịch\n",
    "            tables = soup.find_all('table')\n",
    "            trading_table = None\n",
    "            \n",
    "            for table in tables:\n",
    "                table_text = table.get_text()\n",
    "                if \"MUA\" in table_text and \"BÁN\" in table_text:\n",
    "                    trading_table = table\n",
    "                    break\n",
    "            \n",
    "            if trading_table:\n",
    "                rows = trading_table.find_all('tr')\n",
    "                \n",
    "                # Skip header, lấy 5 rows đầu\n",
    "                for row in rows[1:6]:\n",
    "                    cells = row.find_all('td')\n",
    "                    if len(cells) >= 4:\n",
    "                        buy_price = cells[0].get_text(strip=True)\n",
    "                        buy_volume = cells[1].get_text(strip=True)\n",
    "                        sell_price = cells[2].get_text(strip=True)\n",
    "                        sell_volume = cells[3].get_text(strip=True)\n",
    "                        \n",
    "                        if buy_price and buy_volume:\n",
    "                            trading_data.buy_orders.append({\n",
    "                                \"price\": buy_price,\n",
    "                                \"volume\": buy_volume\n",
    "                            })\n",
    "                        \n",
    "                        if sell_price and sell_volume:\n",
    "                            trading_data.sell_orders.append({\n",
    "                                \"price\": sell_price,\n",
    "                                \"volume\": sell_volume\n",
    "                            })\n",
    "                \n",
    "                # Thông tin nước ngoài\n",
    "                foreign_buy_element = soup.select_one(\"#foreigner_buy_volume\")\n",
    "                foreign_sell_element = soup.select_one(\"#foreigner_sell_volume\")\n",
    "                \n",
    "                if foreign_buy_element:\n",
    "                    trading_data.foreign_buy = foreign_buy_element.get_text(strip=True)\n",
    "                if foreign_sell_element:\n",
    "                    trading_data.foreign_sell = foreign_sell_element.get_text(strip=True)\n",
    "            \n",
    "            return trading_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting trading data for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_financial_statements(self, symbol: str, soup: BeautifulSoup = None) -> List[FinancialStatement]:\n",
    "        \"\"\"Crawl báo cáo tài chính\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"📋 Extracting financial statements for {symbol.upper()}\")\n",
    "            \n",
    "            statements = []\n",
    "            \n",
    "            # Tìm bảng báo cáo tài chính\n",
    "            financial_table = soup.find('table', {'id': 'financial_brief'})\n",
    "            \n",
    "            if not financial_table:\n",
    "                return []\n",
    "            \n",
    "            rows = financial_table.find_all('tr')\n",
    "            if len(rows) < 2:\n",
    "                return []\n",
    "            \n",
    "            # Header row - các periods\n",
    "            header_row = rows[0]\n",
    "            period_cells = header_row.find_all('td')[1:]  # Skip first cell\n",
    "            periods = [cell.get_text(strip=True) for cell in period_cells]\n",
    "            \n",
    "            # Data rows\n",
    "            data_rows = {}\n",
    "            for row in rows[1:]:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) > 1:\n",
    "                    indicator = cells[0].get_text(strip=True)\n",
    "                    values = [cell.get_text(strip=True) for cell in cells[1:]]\n",
    "                    data_rows[indicator] = values\n",
    "            \n",
    "            # Tạo FinancialStatement cho mỗi period\n",
    "            for i, period in enumerate(periods):\n",
    "                statement = FinancialStatement(\n",
    "                    symbol=symbol.upper(),\n",
    "                    period=period\n",
    "                )\n",
    "                \n",
    "                # Map indicators to fields\n",
    "                for indicator, values in data_rows.items():\n",
    "                    if i < len(values):\n",
    "                        value = values[i]\n",
    "                        \n",
    "                        if \"Doanh thu\" in indicator:\n",
    "                            statement.revenue = value\n",
    "                        elif \"Tổng lợi nhuận trước thuế\" in indicator:\n",
    "                            statement.profit_before_tax = value\n",
    "                        elif \"Lợi nhuận sau thuế\" in indicator and \"công ty mẹ\" not in indicator:\n",
    "                            statement.net_profit = value\n",
    "                        elif \"Lợi nhuận sau thuế của công ty mẹ\" in indicator:\n",
    "                            statement.parent_profit = value\n",
    "                        elif \"Tổng tài sản\" in indicator:\n",
    "                            statement.total_assets = value\n",
    "                        elif \"Tổng nợ\" in indicator:\n",
    "                            statement.total_debt = value\n",
    "                        elif \"Vốn chủ sở hữu\" in indicator:\n",
    "                            statement.owner_equity = value\n",
    "                \n",
    "                statements.append(statement)\n",
    "            \n",
    "            return statements\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting financial statements for {symbol}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def crawl_business_plan(self, symbol: str, soup: BeautifulSoup = None) -> List[BusinessPlan]:\n",
    "        \"\"\"Crawl kế hoạch kinh doanh\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"📅 Extracting business plan for {symbol.upper()}\")\n",
    "            \n",
    "            plans = []\n",
    "            \n",
    "            # Tìm section kế hoạch kinh doanh\n",
    "            business_plan_div = soup.find('div', {'id': 'business_plan'})\n",
    "            \n",
    "            if not business_plan_div:\n",
    "                return []\n",
    "            \n",
    "            table = business_plan_div.find('table')\n",
    "            if not table:\n",
    "                return []\n",
    "            \n",
    "            rows = table.find_all('tr')[1:]  # Skip header\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 5:\n",
    "                    year = cells[0].get_text(strip=True)\n",
    "                    revenue_plan = cells[1].get_text(strip=True)\n",
    "                    revenue_achievement = cells[2].get_text(strip=True)\n",
    "                    profit_plan = cells[3].get_text(strip=True)\n",
    "                    profit_achievement = cells[4].get_text(strip=True)\n",
    "                    \n",
    "                    plan = BusinessPlan(\n",
    "                        symbol=symbol.upper(),\n",
    "                        year=year,\n",
    "                        revenue_plan=revenue_plan,\n",
    "                        revenue_achievement=revenue_achievement,\n",
    "                        profit_plan=profit_plan,\n",
    "                        profit_achievement=profit_achievement\n",
    "                    )\n",
    "                    plans.append(plan)\n",
    "            \n",
    "            return plans\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting business plan for {symbol}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def crawl_industry_info(self, symbol: str, soup: BeautifulSoup = None) -> Optional[IndustryInfo]:\n",
    "        \"\"\"Crawl thông tin ngành\"\"\"\n",
    "        if not soup:\n",
    "            url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "            soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"🏭 Extracting industry info for {symbol.upper()}\")\n",
    "            \n",
    "            industry_info = IndustryInfo(symbol=symbol.upper())\n",
    "            \n",
    "            # Tìm thông tin ngành từ h2 \"Ngành/Nhóm/Họ\"\n",
    "            h2_elements = soup.find_all('h2')\n",
    "            \n",
    "            for h2 in h2_elements:\n",
    "                if \"Ngành/Nhóm/Họ\" in h2.get_text():\n",
    "                    # Tìm table ngay sau h2\n",
    "                    next_sibling = h2.find_next_sibling()\n",
    "                    if next_sibling:\n",
    "                        table = next_sibling.find('table')\n",
    "                        if table:\n",
    "                            cell = table.find('td')\n",
    "                            if cell:\n",
    "                                text = cell.get_text(strip=True)\n",
    "                                lines = text.split('\\n')\n",
    "                                \n",
    "                                if len(lines) >= 1:\n",
    "                                    industry_info.market_name = lines[0].strip()\n",
    "                                if len(lines) >= 2:\n",
    "                                    # Loại bỏ phần trong ngoặc\n",
    "                                    industry_name = lines[1].strip()\n",
    "                                    if '(' in industry_name:\n",
    "                                        industry_name = industry_name.split('(')[0].strip()\n",
    "                                    industry_info.industry_name = industry_name\n",
    "                    break\n",
    "            \n",
    "            return industry_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting industry info for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_company_profile(self, symbol: str) -> Optional[CompanyProfile]:\n",
    "        \"\"\"Crawl thông tin chi tiết công ty từ trang profile\"\"\"\n",
    "        url = f\"{self.urls['profile']}{symbol.upper()}\"\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"🏢 Extracting company profile for {symbol.upper()}\")\n",
    "            \n",
    "            profile = CompanyProfile(symbol=symbol.upper())\n",
    "            \n",
    "            # Tìm các thông tin trong table hoặc div chứa thông tin công ty\n",
    "            # Cấu trúc có thể thay đổi, cần flexible parsing\n",
    "            \n",
    "            tables = soup.find_all('table')\n",
    "            for table in tables:\n",
    "                rows = table.find_all('tr')\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    if len(cells) >= 2:\n",
    "                        label = cells[0].get_text(strip=True).lower()\n",
    "                        value = cells[1].get_text(strip=True)\n",
    "                        \n",
    "                        if \"tên đầy đủ\" in label or \"tên công ty\" in label:\n",
    "                            profile.full_name = value\n",
    "                        elif \"tên tiếng anh\" in label:\n",
    "                            profile.english_name = value\n",
    "                        elif \"tên viết tắt\" in label:\n",
    "                            profile.short_name = value\n",
    "                        elif \"địa chỉ\" in label:\n",
    "                            profile.address = value\n",
    "                        elif \"điện thoại\" in label:\n",
    "                            profile.phone = value\n",
    "                        elif \"fax\" in label:\n",
    "                            profile.fax = value\n",
    "                        elif \"website\" in label:\n",
    "                            profile.website = value\n",
    "                        elif \"email\" in label:\n",
    "                            profile.email = value\n",
    "                        elif \"ngày thành lập\" in label:\n",
    "                            profile.established_date = value\n",
    "                        elif \"ngày niêm yết\" in label:\n",
    "                            profile.listed_date = value\n",
    "                        elif \"vốn điều lệ\" in label:\n",
    "                            profile.chartered_capital = value\n",
    "                        elif \"giấy phép kinh doanh\" in label:\n",
    "                            profile.business_license = value\n",
    "                        elif \"mã số thuế\" in label:\n",
    "                            profile.tax_code = value\n",
    "            \n",
    "            return profile\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting company profile for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_complete_stock_data(self, symbol: str) -> CompleteStockData:\n",
    "        \"\"\"Crawl tất cả dữ liệu của một cổ phiếu\"\"\"\n",
    "        logger.info(f\"🎯 Starting complete crawl for {symbol.upper()}\")\n",
    "        \n",
    "        # Lấy soup từ trang summary một lần để tái sử dụng\n",
    "        summary_url = f\"{self.urls['summary']}{symbol.upper()}\"\n",
    "        soup = self.get_soup(summary_url)\n",
    "        \n",
    "        complete_data = CompleteStockData()\n",
    "        \n",
    "        if soup:\n",
    "            # Crawl tất cả dữ liệu từ trang summary\n",
    "            complete_data.basic_info = self.crawl_basic_info(symbol)\n",
    "            complete_data.financial_ratios = self.crawl_financial_ratios(symbol, soup)\n",
    "            complete_data.balance_sheet = self.crawl_balance_sheet(symbol, soup)\n",
    "            complete_data.power_ratings = self.crawl_power_ratings(symbol, soup)\n",
    "            complete_data.trading_data = self.crawl_trading_data(symbol, soup)\n",
    "            complete_data.financial_statements = self.crawl_financial_statements(symbol, soup)\n",
    "            complete_data.business_plans = self.crawl_business_plan(symbol, soup)\n",
    "            complete_data.industry_info = self.crawl_industry_info(symbol, soup)\n",
    "        \n",
    "        # Crawl company profile từ trang riêng\n",
    "        complete_data.company_profile = self.crawl_company_profile(symbol)\n",
    "        \n",
    "        logger.info(f\"✅ Completed crawl for {symbol.upper()}\")\n",
    "        return complete_data\n",
    "    \n",
    "    def crawl_multiple_stocks(self, symbols: List[str], max_workers: int = 5) -> Dict[str, CompleteStockData]:\n",
    "        \"\"\"Crawl nhiều cổ phiếu song song\"\"\"\n",
    "        logger.info(f\"🚀 Starting batch crawl for {len(symbols)} symbols\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Sử dụng ThreadPoolExecutor để crawl song song\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit tất cả tasks\n",
    "            future_to_symbol = {\n",
    "                executor.submit(self.crawl_complete_stock_data, symbol): symbol \n",
    "                for symbol in symbols\n",
    "            }\n",
    "            \n",
    "            # Collect results\n",
    "            for future in concurrent.futures.as_completed(future_to_symbol):\n",
    "                symbol = future_to_symbol[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    results[symbol.upper()] = data\n",
    "                    logger.info(f\"✅ Completed {symbol.upper()}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Error crawling {symbol}: {e}\")\n",
    "                    results[symbol.upper()] = CompleteStockData()\n",
    "        \n",
    "        logger.info(f\"🎉 Batch crawl completed: {len(results)} symbols processed\")\n",
    "        return results\n",
    "    \n",
    "    def crawl_market_list(self, market_type: str = \"all\") -> List[str]:\n",
    "        \"\"\"Crawl danh sách mã cổ phiếu từ thị trường\"\"\"\n",
    "        url = f\"{self.urls['market_data']}?market={market_type}\"\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"📋 Extracting stock list from market: {market_type}\")\n",
    "            \n",
    "            symbols = []\n",
    "            \n",
    "            # Tìm links có pattern /quote/summary.php?id=\n",
    "            links = soup.find_all('a', href=re.compile(r'/quote/summary\\.php\\?id='))\n",
    "            \n",
    "            for link in links:\n",
    "                href = link.get('href', '')\n",
    "                match = re.search(r'id=([A-Z0-9]+)', href)\n",
    "                if match:\n",
    "                    symbol = match.group(1).upper()\n",
    "                    if symbol not in symbols:\n",
    "                        symbols.append(symbol)\n",
    "            \n",
    "            logger.info(f\"✅ Found {len(symbols)} symbols in {market_type} market\")\n",
    "            return symbols\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting market list: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def crawl_industry_list(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Crawl danh sách ngành\"\"\"\n",
    "        url = self.urls['categories']\n",
    "        soup = self.get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"🏭 Extracting industry list\")\n",
    "            \n",
    "            industries = []\n",
    "            \n",
    "            # Tìm các links ngành\n",
    "            links = soup.find_all('a', href=re.compile(r'category'))\n",
    "            \n",
    "            for link in links[:20]:  # Giới hạn 20 ngành đầu\n",
    "                try:\n",
    "                    industry_name = link.get_text(strip=True)\n",
    "                    industry_url = link.get('href', '')\n",
    "                    \n",
    "                    if industry_name and industry_url:\n",
    "                        # Đảm bảo URL đầy đủ\n",
    "                        if not industry_url.startswith('http'):\n",
    "                            industry_url = urljoin(self.urls['base_url'], industry_url)\n",
    "                        \n",
    "                        industries.append({\n",
    "                            \"name\": industry_name,\n",
    "                            \"url\": industry_url\n",
    "                        })\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"✅ Found {len(industries)} industries\")\n",
    "            return industries\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting industry list: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def save_results(self, data: Dict[str, CompleteStockData], output_dir: str = \"results\"):\n",
    "        \"\"\"Lưu kết quả ra files\"\"\"\n",
    "        import os\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Lưu từng loại dữ liệu riêng\n",
    "        basic_info_list = []\n",
    "        financial_ratios_list = []\n",
    "        balance_sheet_list = []\n",
    "        power_ratings_list = []\n",
    "        trading_data_list = []\n",
    "        financial_statements_list = []\n",
    "        business_plans_list = []\n",
    "        industry_info_list = []\n",
    "        company_profiles_list = []\n",
    "        \n",
    "        for symbol, stock_data in data.items():\n",
    "            if stock_data.basic_info:\n",
    "                basic_info_list.append(asdict(stock_data.basic_info))\n",
    "            \n",
    "            if stock_data.financial_ratios:\n",
    "                financial_ratios_list.append(asdict(stock_data.financial_ratios))\n",
    "            \n",
    "            if stock_data.balance_sheet:\n",
    "                balance_sheet_list.append(asdict(stock_data.balance_sheet))\n",
    "            \n",
    "            if stock_data.power_ratings:\n",
    "                power_ratings_list.append(asdict(stock_data.power_ratings))\n",
    "            \n",
    "            if stock_data.trading_data:\n",
    "                trading_data_list.append(asdict(stock_data.trading_data))\n",
    "            \n",
    "            if stock_data.financial_statements:\n",
    "                for stmt in stock_data.financial_statements:\n",
    "                    financial_statements_list.append(asdict(stmt))\n",
    "            \n",
    "            if stock_data.business_plans:\n",
    "                for plan in stock_data.business_plans:\n",
    "                    business_plans_list.append(asdict(plan))\n",
    "            \n",
    "            if stock_data.industry_info:\n",
    "                industry_info_list.append(asdict(stock_data.industry_info))\n",
    "            \n",
    "            if stock_data.company_profile:\n",
    "                company_profiles_list.append(asdict(stock_data.company_profile))\n",
    "        \n",
    "        # Lưu ra CSV files\n",
    "        datasets = {\n",
    "            \"basic_info\": basic_info_list,\n",
    "            \"financial_ratios\": financial_ratios_list,\n",
    "            \"balance_sheet\": balance_sheet_list,\n",
    "            \"power_ratings\": power_ratings_list,\n",
    "            \"trading_data\": trading_data_list,\n",
    "            \"financial_statements\": financial_statements_list,\n",
    "            \"business_plans\": business_plans_list,\n",
    "            \"industry_info\": industry_info_list,\n",
    "            \"company_profiles\": company_profiles_list\n",
    "        }\n",
    "        \n",
    "        for name, dataset in datasets.items():\n",
    "            if dataset:\n",
    "                df = pd.DataFrame(dataset)\n",
    "                csv_path = os.path.join(output_dir, f\"{name}.csv\")\n",
    "                df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "                logger.info(f\"💾 Saved {len(dataset)} records to {csv_path}\")\n",
    "        \n",
    "        # Lưu raw data dạng JSON\n",
    "        json_data = {}\n",
    "        for symbol, stock_data in data.items():\n",
    "            json_data[symbol] = {\n",
    "                \"basic_info\": asdict(stock_data.basic_info) if stock_data.basic_info else None,\n",
    "                \"financial_ratios\": asdict(stock_data.financial_ratios) if stock_data.financial_ratios else None,\n",
    "                \"balance_sheet\": asdict(stock_data.balance_sheet) if stock_data.balance_sheet else None,\n",
    "                \"power_ratings\": asdict(stock_data.power_ratings) if stock_data.power_ratings else None,\n",
    "                \"trading_data\": asdict(stock_data.trading_data) if stock_data.trading_data else None,\n",
    "                \"financial_statements\": [asdict(stmt) for stmt in stock_data.financial_statements],\n",
    "                \"business_plans\": [asdict(plan) for plan in stock_data.business_plans],\n",
    "                \"industry_info\": asdict(stock_data.industry_info) if stock_data.industry_info else None,\n",
    "                \"company_profile\": asdict(stock_data.company_profile) if stock_data.company_profile else None\n",
    "            }\n",
    "        \n",
    "        json_path = os.path.join(output_dir, \"complete_data.json\")\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(f\"💾 Saved complete data to {json_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Hàm chính để test crawler\"\"\"\n",
    "    print(\"🚀 Cophieu68 BeautifulSoup Crawler\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Khởi tạo crawler\n",
    "    crawler = Cophieu68BeautifulSoupCrawler(delay=1.0)\n",
    "    \n",
    "    # Test symbols\n",
    "    test_symbols = [\"VCB\", \"VIC\", \"VHM\", \"HPG\", \"MSN\", \"DNN\"]\n",
    "    \n",
    "    try:\n",
    "        # Test crawl một symbol đầy đủ\n",
    "        print(f\"\\n🎯 Test complete crawl for {test_symbols[0]}\")\n",
    "        complete_data = crawler.crawl_complete_stock_data(test_symbols[0])\n",
    "        \n",
    "        # In thông tin cơ bản\n",
    "        if complete_data.basic_info:\n",
    "            print(f\"  ✅ Basic Info: {complete_data.basic_info.company_name}\")\n",
    "            print(f\"     Price: {complete_data.basic_info.current_price}\")\n",
    "            print(f\"     Change: {complete_data.basic_info.price_change} ({complete_data.basic_info.percent_change})\")\n",
    "        \n",
    "        if complete_data.financial_ratios:\n",
    "            print(f\"  ✅ Financial Ratios: PE={complete_data.financial_ratios.pe_ratio}, PB={complete_data.financial_ratios.pb_ratio}\")\n",
    "        \n",
    "        if complete_data.financial_statements:\n",
    "            print(f\"  ✅ Financial Statements: {len(complete_data.financial_statements)} periods\")\n",
    "        \n",
    "        if complete_data.industry_info:\n",
    "            print(f\"  ✅ Industry: {complete_data.industry_info.market_name} - {complete_data.industry_info.industry_name}\")\n",
    "        \n",
    "        # Test batch crawl\n",
    "        print(f\"\\n🚀 Test batch crawl for {len(test_symbols[:3])} symbols\")\n",
    "        batch_results = crawler.crawl_multiple_stocks(test_symbols[:3], max_workers=3)\n",
    "        \n",
    "        for symbol, data in batch_results.items():\n",
    "            status = \"✅\" if data.basic_info else \"❌\"\n",
    "            company_name = data.basic_info.company_name if data.basic_info else \"N/A\"\n",
    "            print(f\"  {status} {symbol}: {company_name}\")\n",
    "        \n",
    "        # Test crawl market list\n",
    "        print(f\"\\n📋 Test market list crawl\")\n",
    "        market_symbols = crawler.crawl_market_list(\"vnall\")\n",
    "        print(f\"  ✅ Found {len(market_symbols)} symbols in market\")\n",
    "        if market_symbols:\n",
    "            print(f\"  📝 Sample: {market_symbols[:10]}\")\n",
    "        \n",
    "        # Test crawl industry list\n",
    "        print(f\"\\n🏭 Test industry list crawl\")\n",
    "        industries = crawler.crawl_industry_list()\n",
    "        print(f\"  ✅ Found {len(industries)} industries\")\n",
    "        for industry in industries[:5]:\n",
    "            print(f\"    - {industry.get('name', 'N/A')}\")\n",
    "        \n",
    "        # Lưu kết quả\n",
    "        print(f\"\\n💾 Saving results...\")\n",
    "        crawler.save_results(batch_results, \"beautifulsoup_results\")\n",
    "        \n",
    "        print(f\"\\n🎉 Test completed successfully!\")\n",
    "        print(f\"📁 Check results in 'beautifulsoup_results/' folder\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error in main: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ee7e9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
